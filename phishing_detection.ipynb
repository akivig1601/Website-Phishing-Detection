{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4peoMaAp1SYy49kkZOXcC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akivig1601/Website-Phishing-Detection/blob/main/phishing_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Imports and Environment Setup"
      ],
      "metadata": {
        "id": "cWUjOQ9aLdFT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM1IY4klNur9",
        "outputId": "9c5e7ceb-1ceb-4754-99d2-6ad243733620",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost: OK\n",
            "shap: OK\n"
          ]
        }
      ],
      "source": [
        "import subprocess, sys\n",
        "\n",
        "# Install required packages\n",
        "for package in ['xgboost', 'optuna', 'shap', 'scikit-learn', 'seaborn', 'matplotlib', 'pandas', 'numpy']:\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f\"{package}: OK\")\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, sys\n",
        "\n",
        "# Install required packages\n",
        "for package in ['xgboost', 'optuna', 'shap', 'scikit-learn', 'seaborn', 'matplotlib', 'pandas', 'numpy']:\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f\"{package}: OK\")\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n"
      ],
      "metadata": {
        "id": "FfWidr4UiSf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“¥ 1. Load Dataset"
      ],
      "metadata": {
        "id": "m-zbxFFWLne6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import arff\n",
        "data, meta = arff.loadarff('Training Dataset.arff')\n",
        "df = pd.DataFrame(data).applymap(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
        "df = df.apply(pd.to_numeric, errors='coerce').dropna()\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "qjIvymvWOiqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. EDA and Feature Engineering"
      ],
      "metadata": {
        "id": "Xk55dXf_TQCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA"
      ],
      "metadata": {
        "id": "suYi1_J3iYuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset shape:\", df.shape)\n",
        "print(df[\"Result\"].value_counts())\n",
        "sns.countplot(x=df[\"Result\"].map({-1: 'Phishing', 1: 'Legitimate'}))\n",
        "plt.title(\"Class Distribution\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df.drop('Result', axis=1).corr(), cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eKm-DpPQPkDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering: N-gram features"
      ],
      "metadata": {
        "id": "diix9ch3ieFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synthesize URL-like strings from features:"
      ],
      "metadata": {
        "id": "Xk9PUA0Iifut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_url_features(row):\n",
        "    url_parts = []\n",
        "    if 'having_Sub_Domain' in row.index and row['having_Sub_Domain'] == 1:\n",
        "        url_parts.append('subdomain.suspicious')\n",
        "    if 'URL_Length' in row.index:\n",
        "        l = row['URL_Length']\n",
        "        if l > 75:\n",
        "            url_parts.append('verylongurl')\n",
        "        elif l > 54:\n",
        "            url_parts.append('longurl')\n",
        "    if 'having_At_Symbol' in row.index and row['having_At_Symbol'] == 1:\n",
        "        url_parts.append('at@symbol')\n",
        "    if 'double_slash_redirecting' in row.index and row['double_slash_redirecting'] == 1:\n",
        "        url_parts.append('redirect//')\n",
        "    return '.'.join(url_parts) if url_parts else 'standard.url.com'\n",
        "\n",
        "df['synthetic_url'] = df.drop('Result', axis=1).apply(create_url_features, axis=1)\n"
      ],
      "metadata": {
        "id": "V9GEtAXyijaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract n-gram features:"
      ],
      "metadata": {
        "id": "Mv0vo-CyTWMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def extract_ngram_features(urls, ngram_range=(2, 4), max_features=40):\n",
        "    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=ngram_range, max_features=max_features, lowercase=True)\n",
        "    ngram_matrix = vectorizer.fit_transform(urls)\n",
        "    feature_names = [f'ngram_{name}' for name in vectorizer.get_feature_names_out()]\n",
        "    return ngram_matrix.toarray(), feature_names\n",
        "\n",
        "ngram_features, ngram_names = extract_ngram_features(df['synthetic_url'])\n",
        "ngram_df = pd.DataFrame(ngram_features, columns=ngram_names, index=df.index)\n"
      ],
      "metadata": {
        "id": "i_LMmXYoPOkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Preprocessing"
      ],
      "metadata": {
        "id": "mmRuz0PnTbHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_original = df.drop(columns=[\"Result\", \"synthetic_url\"])\n",
        "X_enhanced = pd.concat([X_original, ngram_df], axis=1)\n",
        "y = df[\"Result\"]\n",
        "y_binary = y.map({-1: 0, 1: 1})  # For XGBoost & ML compatibility\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_enhanced, y_binary, stratify=y_binary, test_size=0.2, random_state=RANDOM_STATE\n",
        ")"
      ],
      "metadata": {
        "id": "SIatwkccPwar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Model Definitions and Training"
      ],
      "metadata": {
        "id": "jE-1oEOWTiip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Logistic Regression\": Pipeline([\n",
        "        ('scale', StandardScaler()),\n",
        "        ('clf', LogisticRegression())\n",
        "    ]),\n",
        "    \"Decision Tree\": Pipeline([\n",
        "        ('clf', DecisionTreeClassifier())\n",
        "    ]),\n",
        "    \"Random Forest\": Pipeline([\n",
        "        ('clf', RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE))\n",
        "    ]),\n",
        "    \"SVM (RBF)\": Pipeline([\n",
        "        ('scale', StandardScaler()),\n",
        "        ('clf', SVC(kernel='rbf', probability=True))\n",
        "    ]),\n",
        "    \"Gradient Boosting\": Pipeline([\n",
        "        ('clf', GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "IigELkM8PzO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and Evaluate"
      ],
      "metadata": {
        "id": "8P8idP9bi0NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "for name, pipe in models.items():\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    y_proba = pipe.predict_proba(X_test)[:,1] if hasattr(pipe.named_steps['clf'], 'predict_proba') else None\n",
        "\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"F1\": classification_report(y_test, y_pred, output_dict=True)[str(1)][\"f1-score\"] if \"1\" in classification_report(y_test, y_pred, output_dict=True) else 0,\n",
        "        \"ROC AUC\": roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
        "    }\n",
        "\n",
        "results_df = pd.DataFrame(results).T.sort_values(\"Accuracy\", ascending=False)\n",
        "print(results_df.round(4))\n"
      ],
      "metadata": {
        "id": "cNHwMRQzizVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. XGBoost With Optuna Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "b1Cuk2Q-i3uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'logloss',\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 200),\n",
        "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 3),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 3),\n",
        "        'random_state': RANDOM_STATE,\n",
        "        'verbosity': 0\n",
        "    }\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10, show_progress_bar=True)\n",
        "optimized_xgb = xgb.XGBClassifier(**study.best_params)\n",
        "optimized_xgb.fit(X_train, y_train)\n",
        "models[\"Optimized XGBoost\"] = Pipeline([('clf', optimized_xgb)])"
      ],
      "metadata": {
        "id": "qOiQK4P6LkUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. SHAP Explainability"
      ],
      "metadata": {
        "id": "osaAz1JNTpFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_sample = X_test.iloc[:30].values\n",
        "feature_names = X_test.columns.tolist()\n",
        "explainer = shap.TreeExplainer(optimized_xgb)\n",
        "shap_values = explainer.shap_values(X_test_sample)\n",
        "feature_importance_shap = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': np.abs(shap_values).mean(axis=0)\n",
        "}).sort_values('importance', ascending=False)\n",
        "print(feature_importance_shap.head(10))\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_shap['feature'][:10], feature_importance_shap['importance'][:10])\n",
        "plt.xlabel('Mean |SHAP Value|')\n",
        "plt.title('Top 10 SHAP Features')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DWcwcYksTtrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Stacking Ensemble"
      ],
      "metadata": {
        "id": "pUgdrbHwTwdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "base_learners = [\n",
        "    ('lr', Pipeline([('scale', StandardScaler()), ('clf', LogisticRegression())])),\n",
        "    ('rf', Pipeline([('clf', RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE))])),\n",
        "    ('xgb', Pipeline([('clf', optimized_xgb)])),\n",
        "    ('svm', Pipeline([('scale', StandardScaler()), ('clf', SVC(kernel=\"rbf\", probability=True))]))\n",
        "]\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_learners,\n",
        "    final_estimator=LogisticRegression(random_state=RANDOM_STATE),\n",
        "    cv=5,\n",
        "    stack_method='predict_proba',\n",
        "    n_jobs=-1\n",
        ")\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "print('Stacking Ensemble Accuracy:', accuracy_score(y_test, y_pred_stack))\n",
        "models[\"Stacking Ensemble\"] = stacking_clf"
      ],
      "metadata": {
        "id": "RS4Jou1yTyTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Performance Visualization"
      ],
      "metadata": {
        "id": "33FkaIBcT2kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "results_enhanced = {}\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "    results_enhanced[name] = {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"F1\": classification_report(y_test, y_pred, output_dict=True)['1'][\"f1-score\"] if \"1\" in classification_report(y_test, y_pred, output_dict=True) else 0,\n",
        "        \"ROC AUC\": roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n",
        "    }\n",
        "results_df = pd.DataFrame(results_enhanced).T.round(4)\n",
        "results_df.plot(kind='bar', figsize=(15,6))\n",
        "plt.title(\"Model Performance Comparison\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OneWl5TwT4Kr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}